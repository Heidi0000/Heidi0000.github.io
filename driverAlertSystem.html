<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Driver Alert System Page</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://kit.fontawesome.com/69fa28cb17.js" crossorigin="anonymous"></script>
    <script>
        function scrollFunction(thing, n){
            var elmnt = document.getElementsByClassName(thing)[n];
            elmnt.scrollIntoView({top: 10, behavior: 'smooth'});
        }
    </script>
</head>
<body>
    <!--navigation bar-->
    <div class="nav-wrapper">
        <div class="left-side">
            <div class="brand">
                <div>HEIDI HAN</div>
            </div>
        </div>

        <div class="right-side">
            <div class="nav-link-wrapper">
                <button onclick="window.location.href='index.html'">Home</button>
            </div>
            <div class="nav-link-wrapper">
                <!--<a href="timeline-containter">Projects</a>-->
                <button onclick="window.location.href='index.html#proj-for-navbar'">Projects</button>
            </div>
            <div class="nav-link-wrapper">
                <!-- <a href="project1.html">Experience</a> -->
                <button onclick="window.location.href='index.html#work-for-navbar'">Experiences</button>
            </div>
            <div class="nav-link-wrapper">
                <button onclick="scrollFunction('contact-container', 0)">Contact</a>
            </div>
            <!-- <div class="nav-link-wrapper">
                <a> | </a>
            </div> -->
            <div class="nav-link-wrapper">
                <button onclick="window.location.href='moreaboutme.html'">About Me</button>
            </div>
            <div class="nav-link-wrapper">
                <button><a href="images/2b_resume_robotics.pdf" download >Resume</a></button>
                <!-- <img src="images/direct-download.png" alt="Project 1" style="object-fit: cover;width:1.1rem; height:1.1rem; position: relative;"> -->
            </div>

        </div>

    </div>
    <div class="proj-page-wrapper">
        
        <div class="proj-page-name">Driver Alert System</div>
        <h4 class="short-desc-proj">Personal Robotics Project 
            completed with <a class="linkk" href="https://www.linkedin.com/in/billy-kim/?originalSubdomain=ca">Billy Kim</a>
        </h4>

        <div class="demo-image-container">
            <video width="100%" autoplay loop muted>
                <source src="images/demo-driver-alert-sys.webm" type="video/webm">
            </video>
            <h6 class="demo-caption">Demo of Driver Alert System</h6>
        </div>

        <div class="proj-content-wrapper">
            <h3 class="subheader-proj">Summary</h3>
            <p>The Driver Alert System is a program written in Python to alert a drowsy driver when their eyes are closed.
                It uses OpenCV and machine learning to determine if the driver's eyes are open or not, and the live 
                classification is streamed using Qt. 
            </p>
            <div class="tools">
                <p>TOOLS:  </p>
                <ul class="tools-list">
                    <li>Python</li>
                    <li>OpenCV</li>
                    <li>TensorFlow</li>
                    <li>Keras</li>
                    <li>Qt</li>
                </ul>
            </div>
            <h3 class="subheader-proj">Building it</h3>
            <h4 class="design-proj-heading">1. Detect presence and location of eyes</h4>

            <p> The first step in this project is using OpenCV to detect faces, more specifically, the eyes, so the 
                machine can determine if the driver's eyes are closed or not. 
                Haar cascade is a commonly used object detection method where 
                a cascade function is trained from a lot of positive and negative images.
                Typically, if one wants to train their own classifier for specific objects, 
                that can be done, but in our case, OpenCV already contained pre-trained classifiers for the face and eyes. 
                To use these classifiers, simply load it. 
            </p>
            <p class="code">  face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
                <br> eye_cascade = cv2.CascadeClassifier('haarcascade_lefteye_2splits.xml')
            </p>
            <p>
                Once the classifiers are loaded, we can then use the detectMultiScale function to detect the face/eye, 
                which is going to return the position of the face as a rectangle (Rect(x, y, w, h)). With these points, we can then 
                create an ROI (region of interest or bounding box) around the eyes.
            </p>
            <p class="code">
                faces = face_cascade.detectMultiScale(image, 1.3, 5)
                <br>roi_eyes = [] 
                <br>for (x,y,w,h) in faces:
                <br>&nbsp;&nbsp;&nbsp;&nbsp;   roi_gray = image[y:y+h, x:x+w]
                <br>&nbsp;&nbsp;&nbsp;&nbsp;   roi_color = image[y:y+h, x:x+w]
                <br>&nbsp;&nbsp;&nbsp;&nbsp;   eyes = eye_cascade.detectMultiScale(roi_gray)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;    dsize = (80,80)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;    for (ex, ey, ew, eh) in eyes:
                <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        resolutionImage = cv2.resize(roi_gray[ey:ey+eh, ex:ex+ew], dsize, interpolation = cv2.INTER_AREA)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        roi_eyes.append(resolutionImage)  
            </p>
            <br>
            
            <h4 class="design-proj-heading">2. Determine if the eyes retrieved from step 1 are closed</h4>

            <p>
                This part uses Tensorflow and Keras models to determine if the eye images retrieved from the above step
                are open or closed. To begin this step, first a dataset of open and closed eyes must be collected. 
                For this project, 30,000 images of open and closed eyees were collected from <a class="linkk" href=" http://mrl.cs.vsb.cz/eyedataset">here</a>. 
                20,000 of these were used for the training, and 10,000 were used for testing. 
            </p>
            <p>After some pre-processing of the images, layers must be set up. Layers are the basic building blocks of neural networks in Keras.
                A layer consists of a tensor-in tensor-out computation function and some state. Most layers have some parameters which are learned
                during the training. For this project, 3 layers were used: 1 flatten and 2 dense layers. The flattening layer takes a 2d array of pixels 
                and unstacks the rows into a 1d array and was used for data reformatting purposes. The densing layers return an array with length of 
                num_nodes, which is a parameter passed in. 
            </p>
            <p class="code">  
                model = keras.Sequential([
                <br>&nbsp;&nbsp;&nbsp;&nbsp; keras.layers.Flatten(input_shape=(70,70)),
                <br>&nbsp;&nbsp;&nbsp;&nbsp; keras.layers.Dense(128, activation='relu'),
                <br>&nbsp;&nbsp;&nbsp;&nbsp; keras.layers.Dense(2)])
            </p>
            <p>
                The next step is to compile the model. This part is done using a few functions:
                <br>&nbsp;&nbsp;&nbsp;&nbsp;• loss function -measures the accuracy of the model during training, 
                <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; •we want to minimize this function and it will "steer" the model in the right direction
                <br>&nbsp;&nbsp;&nbsp;&nbsp;•  optimizer - how the model is updated based on the data it sees and the loss function
                <br>&nbsp;&nbsp;&nbsp;&nbsp;•  metrics - monitors the training and testing steps

            </p>
            <p class="code">    model.compile(optimizer='adam', #adam is some algorithm
                <br>&nbsp;&nbsp;&nbsp;&nbsp;loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
                <br>&nbsp;&nbsp;&nbsp;&nbsp;metrics=['accuracy'])
            </p>
            <p>
                Then comes the training of the model. This is done in one line, in four steps:
                <br>&nbsp;&nbsp;&nbsp;&nbsp;     1. feed the training data (train_images and train_labels) into the model
                 <br>&nbsp;&nbsp;&nbsp;&nbsp;   2. model will learn to associate images and labels
                   <br> &nbsp;&nbsp;&nbsp;&nbsp;        3. ask the model to make predictions about a test set (test_images)
                  <br>&nbsp;&nbsp;&nbsp;&nbsp;  4. check that the predictions match the test_labels
            </p>
            <p class="code">    model.fit(train_images, train_labels, epochs=10)
            </p>
            <p>With every epoch, the loss/accuracy should improve (decrease/increase).</p>
            <p>Now moving on to the testing part, the accuracy is evaluated by trying the model on a test set:</p>
            <p class="code">    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
            </p>
            <p>And finally, the model is now ready to be used. To used this model, once the image to be classified is 
                inputted, the output will be an array of predictions, where the predictions are  the confidence levels
                 of the image being the thing at the index. For instance, if returned an array of [0.9 0.2], where index 0 
                 is for closed eye and 1 is open, then there is a 90 percent chance that the inputted image is a closed eye.
            </p>

            <h4 class="design-proj-heading">3. Put them together in a nice GUI!</h4>

            <p> This part was done using PyQt5. First, in order to display a live stream of the webcam, 
                a multi threaded program was required, as otherwise the GUI would keep freezing and not be able
                to keep up with the work it must do. So a main loop was established, inheriting from QWidget, then
                the worker thread was created. The main loop was responsible 
                The next step then was to add in the machine learning step. This was done in the worker thread, where 
                it would take each frame and determine if the eyes are closed or not, and then emit a signal to the main thread.
            </p>
            <div class="proj-bottom-container"> 
                <button class="view-proj-code-btn"><a href="https://github.com/ih4kim/DriverAlertSystem">View code in Github</a></button>
            </div>
                
        </div>
    </div>
    <div class="bottom-footer">
        <div class="contact-container">
            <!-- <div id="contact-box"> -->
                <p>© 2020 Heidi Han</p>
                <ul>
                    <li>
                        <a href="mailto:d55han@uwaterloo.ca" class="contact-icons"><i class="far fa-envelope"></i></a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/heidi0000" class="contact-icons"><i class="fab fa-linkedin"></i></a>
                    </li>
                    <li>
                        <a href="https://www.github.com/Heidi0000" class="contact-icons"><i class="fab fa-github"></i></a>
                    </li>
                    
                </ul>
            <!-- </div> -->
        </div>
    </div>
</body>
</html>